{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Tutorial\n",
    "\n",
    "In this tutorial, we will demonstrate how to use aero to perform ASR tasks as well as batch inference\n",
    "\n",
    "### Envs\n",
    "\n",
    "You can prepare the basic envs by installing the following packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install huggingface_hub librosa torch accelerate\n",
    "# !pip install transformers@git+https://github.com/huggingface/transformers@v4.51.3-Qwen2.5-Omni-preview\n",
    "# !pip install --no-build-isolation flash-attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QuickStart\n",
    "\n",
    "In this example, we demonstrate the basic usage of our model. The whole processing and generation logic is pure transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "\n",
    "import torch\n",
    "import librosa\n",
    "\n",
    "def load_audio():\n",
    "    return librosa.load(librosa.ex(\"libri1\"), sr=16000)[0]\n",
    "\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"lmms-lab/Aero-1-Audio-1.5B\", trust_remote_code=True)\n",
    "# We encourage to use flash attention 2 for better performance\n",
    "# Please install it with `pip install --no-build-isolation flash-attn`\n",
    "# If you do not want flash attn, please use sdpa or eager`\n",
    "model = AutoModelForCausalLM.from_pretrained(\"lmms-lab/Aero-1-Audio-1.5B\", device_map=\"cuda\", torch_dtype=\"auto\", attn_implementation=\"flash_attention_2\", trust_remote_code=True)\n",
    "model.eval()\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"audio_url\",\n",
    "                \"audio\": \"placeholder\",\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Please transcribe the audio\",\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "audios = [load_audio()]\n",
    "\n",
    "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(text=prompt, audios=audios, sampling_rate=16000, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "outputs = model.generate(**inputs, eos_token_id=151645, max_new_tokens=4096)\n",
    "\n",
    "cont = outputs[:, inputs[\"input_ids\"].shape[-1] :]\n",
    "\n",
    "print(processor.batch_decode(cont, skip_special_tokens=True)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Inference\n",
    "\n",
    "Our model also supports batch inference. By padding the inputs and setting the padding side to left, we can accelerate our inference with large batch size. Thanks to the small size of the model, you can easily run batch size up to 32 batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "\n",
    "import torch\n",
    "import librosa\n",
    "\n",
    "def load_audio():\n",
    "    return librosa.load(librosa.ex(\"libri1\"), sr=16000)[0]\n",
    "\n",
    "def load_audio_2():\n",
    "    return librosa.load(librosa.ex(\"libri2\"), sr=16000)[0]\n",
    "\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"lmms-lab/Aero-1-Audio-1.5B\", trust_remote_code=True)\n",
    "# We encourage to use flash attention 2 for better performance\n",
    "# Please install it with `pip install --no-build-isolation flash-attn`\n",
    "# If you do not want flash attn, please use sdpa or eager`\n",
    "model = AutoModelForCausalLM.from_pretrained(\"lmms-lab/Aero-1-Audio-1.5B\", device_map=\"cuda\", torch_dtype=\"auto\", attn_implementation=\"flash_attention_2\", trust_remote_code=True)\n",
    "model.eval()\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"audio_url\",\n",
    "                \"audio\": \"placeholder\",\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Please transcribe the audio\",\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "messages = [messages, messages]\n",
    "\n",
    "audios = [load_audio(), load_audio_2()]\n",
    "\n",
    "processor.tokenizer.padding_side=\"left\"\n",
    "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(text=prompt, audios=audios, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "outputs = model.generate(**inputs, eos_token_id=151645, pad_token_id=151643, max_new_tokens=4096)\n",
    "\n",
    "cont = outputs[:, inputs[\"input_ids\"].shape[-1] :]\n",
    "\n",
    "print(processor.batch_decode(cont, skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
